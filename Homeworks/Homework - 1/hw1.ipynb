{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96dd0c0e",
   "metadata": {},
   "source": [
    "# STA 208: Homework 1 (Do not distribute)\n",
    "\n",
    "## Due 04/24/2022 midnight (11:59pm)\n",
    "\n",
    "__Instructions:__ \n",
    "\n",
    "1. Submit your homework using one file name ”LastName_FirstName hw1.html” on canvas. \n",
    "2. The written portions can be either done in markdown and TeX in new cells or written by hand and scanned. Using TeX is strongly preferred. However, if you have scanned solutions for handwriting, you can submit a zip file. Please make sure your handwriting is clear and readable and your scanned files are displayed properly in your jupyter notebook. \n",
    "3. Your code should be readable; writing a piece of code should be compared to writing a page of a book. Adopt the one-statement-per-line rule. Consider splitting a lengthy statement into multiple lines to improve readability. (You will lose one point for each line that does not follow the one-statementper-line rule)\n",
    "4. To help understand and maintain code, you should always add comments to explain your code. (homework with no comments will receive 0 points). For a very long comment, please break it into multiple lines.\n",
    "5. In your Jupyter Notebook, put your answers in new cells after each exercise. You can make as many new cells as you like. Use code cells for code and Markdown cells for text.\n",
    "6. Please make sure to print out the necessary results to avoid losing points. We should not run your code to figure out your answers. \n",
    "7. However, also make sure we are able to open this notebook and run everything here by running the cells in sequence; in case that the TA wants to check the details.\n",
    "8. You will be graded on correctness of your math, code efficiency and succinctness, and conclusions and modelling decisions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43772f97",
   "metadata": {},
   "source": [
    "### Exercise 1 (Empirical risk minimization) (20 pts, 5 pts each)\n",
    "\n",
    "Consider Poisson model with rate parameter $\\lambda$ which has PMF,\n",
    "$$\n",
    "p(y|\\lambda) = \\frac{\\lambda^y}{y!} e^{-\\lambda},\n",
    "$$\n",
    "where $y = 0,1,\\ldots$ is some count variable.\n",
    "In Poison regression, we model $\\lambda = e^{\\beta^\\top x}$ to obtain $p(y | x,\\beta)$.\n",
    "\n",
    "1. Let the loss function for Poisson regression be $\\ell_i(\\beta) \\propto - \\log p(y_i | x_i, \\beta)$ for a dataset consisting of predictor variables and count values $\\{x_i,y_i\\}_{i=1}^n$.  Here $\\propto$ means that we disregard any additive terms that are not dependent on $\\beta$.  Write an expression for $\\ell_i$ and derive its gradient. \n",
    "\n",
    "__Solution:__\n",
    "\n",
    "Based on the above expression, we can find the log density of the poisson distribution after substituting $\\lambda = e^{\\beta^Tx}$ back into the probability function. On doing so, we get the density function as: \n",
    "\n",
    "$$\n",
    "p(y|x,\\beta) = \\frac{(e^{\\beta^Tx})^y}{y!} e^{-e^{\\beta^Tx}}\n",
    "$$\n",
    "\n",
    "On taking the log of this, we get:\n",
    "\n",
    "$$\n",
    "\\log p(y|x,\\beta) = y\\beta^Tx - \\log(y!) - {e^{\\beta^Tx}}\n",
    "$$\n",
    "\n",
    "Based on the definition of the $\\alpha$ proportionality, we just ignore the additive terms not dependent on $\\beta$ to get the loss function which is given as:\n",
    "\n",
    "$$\n",
    "l_i(\\beta) = -\\log p(y_i | x_i, \\beta) = - (y_i\\beta^Tx_i - e^{\\beta^Tx_i}) = e^{\\beta^Tx_i} - y_i\\beta^Tx_i\n",
    "$$\n",
    "\n",
    "The gradient of this loss function is given as:\n",
    "$$\n",
    "\\frac{\\partial l_i(\\beta)}{\\partial \\beta} = x_i (e^{\\beta^Tx_i} - y_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcb5c38",
   "metadata": {},
   "source": [
    "2. Show that the empirical risk $R_n(\\beta)$ is a convex function of $\\beta$.\n",
    "\n",
    "__Solution:__\n",
    "\n",
    "The risk function $R_n(\\beta)$ is defined as summation of loss functions for each data point. It is formally defined as:\n",
    "\n",
    "$$\n",
    "R_n(\\beta) = \\frac{1}{n} \\sum_{i=1}^n l_i(\\beta)\n",
    "$$\n",
    "\n",
    "In order to show that the risk function is convex, we can use the fact that if loss function is convex, then the risk function is convex as risk function is a linear combination or sum of the loss functions at every 'i'.\n",
    "\n",
    "Now in order to show the loss function is convex, we have to show that the double derivative of the loss function is $\\geq 0$.\n",
    "\n",
    "From part 1, we have the gradient as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l_i(\\beta)}{\\partial \\beta} = x_i (e^{\\beta^Tx_i} - y_i)\n",
    "$$\n",
    "\n",
    "Taking the double derivate of the above expression as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 l_i(\\beta)}{\\partial \\beta^2} = x_i^Tx_i(e^{\\beta^Tx_i}) \\geq 0\n",
    "$$\n",
    "\n",
    "Since for any values of $x_i$, the double derivative would be greater than 0, we can conclude that the loss function is convex. \n",
    "\n",
    "Since the loss function is convex and the risk function is the sum of loss functions, we can conclude that the risk function is convex as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562daffa",
   "metadata": {},
   "source": [
    "3. Consider the mapping $F_\\eta(\\beta) = \\beta - \\eta \\nabla R_n(\\beta)$ which is the iteration of gradient descent ($\\eta>0$ is called the learning parameter).  Show that at the minimizer of $R_n$, $\\hat \\beta$, we have that $F(\\hat \\beta) = \\hat \\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c66b76",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "4. I have a script to simulate from this model below.  Implement the gradient descent algorithm above and show that with enough data ($n$ large enough) the estimated $\\hat \\beta$ approaches the true $\\beta$ (you can look at the sum of square error between these two vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3ad3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb9d5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simulate from the Poisson regression model (use y,X)\n",
    "np.random.seed(2022)\n",
    "n, p = 1000,20\n",
    "X = np.random.normal(0,1,size = (n,p))\n",
    "beta = np.random.normal(0,.2,size = (p))\n",
    "lamb = np.exp(X @ beta)\n",
    "y = np.random.poisson(lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571e56e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the empirical risk function\n",
    "def risk_n(y,X,beta):\n",
    "    \n",
    "\n",
    "# Defining gradient descent function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360cc371",
   "metadata": {},
   "source": [
    "### Exercise 2 (Regression and OLS) (35 pts, 5 pts each)\n",
    "\n",
    "Consider the regression setting in which $x_i \\in \\mathbb R^p$ and $y_i \\in \\mathbb R$, for $i=1,\\ldots,n$ and $p < n$.\n",
    "\n",
    "1. For a given regressor, let $\\hat y_i$ be prediction given $x_i$, and $\\hat y$ be the vector form.  Show that linear regression can be written in the form\n",
    "$$\n",
    "\\hat y = H y,\n",
    "$$\n",
    "where $H$ is dependent on $X$ (the matrix of where each row is $x_i$), assuming that $p < n$ and $X$ is full rank.  Give an expression for $H$ or an algorithm for computing $H$. \n",
    "2. Assuming $p < n$ and $X$ is full rank, let $X = U D V^\\top$ be the thin singular value decomposition where $U$ is $n \\times p$, and $V, D$ is $p \\times p$ ($D$ is diagonal). \n",
    "    - a) Derive an expression for the OLS coefficients $\\beta = A b$ such that $A$ is $p \\times p$ and depends on $V$ and $D$, and $b$ is a $p$ vector and does not depend on $D$.   \n",
    "    - b) Describe a fit method that precomputes these quantities separately\n",
    "    - c) Use the simulated data $y$ and $X$ in below to find $\\hat \\beta$ using SVD.\n",
    "    - d) Call a new data $\\tilde X \\in \\mathbb{R}^{m \\times p}$, derive an expression for the predicted $y$ with $\\tilde X$ using SVD. \n",
    "3. Consider a regressor that performs OLS using the SVD above, but every instance of D will only use the largest $r$ values on the diagonal, the remainder will be set to 0.  Call this new $p \\times p$ matrix $D_r$ ($r < p$).  Then the new coefficient vector is the OLS computed as if the design matrix is modified by $X \\gets U D_r V^\\top$.  \n",
    "    - a) Given that you have computed $b$ already, how could you make a method `change_rank` that recomputes $A$ with $D_r$ instead of $D$? \n",
    "    - b) Choose $r = 10$, recompute $\\hat\\beta$ (call it $\\hat\\beta_{\\text{LowRank}}$) in Question 2-c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced8a4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simulate from the linear regression model (use y,X)\n",
    "np.random.seed(2022)\n",
    "n, p = 100,20\n",
    "X = np.random.normal(0,1,size = (n,p))\n",
    "beta = np.random.normal(0,.2,size = (p))\n",
    "sigma = 1\n",
    "y = np.random.normal(X @ beta, sigma**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882a747b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.matrix_rank(X) # X is full rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634fe776",
   "metadata": {},
   "source": [
    "### Exercise 3 (Subset selection)  (15 pts)\n",
    "\n",
    "Recall the subset selection problem with tuning parameter $k$,\n",
    "$$\n",
    "\\min_{\\beta : \\| \\beta \\|_0 \\le k}\\| y - X \\beta \\|_2^2,\n",
    "$$\n",
    "where $\\|\\beta\\|_0 = \\#\\{j = 1\\,\\ldots,p : \\beta_j \\ne 0 \\}$.\n",
    "\n",
    "Notice that we can write this as \n",
    "$$\n",
    "\\min_{\\beta : |{\\rm supp}(\\beta)| \\le k}\\| y - X \\beta \\|_2^2,\n",
    "$$\n",
    "where \n",
    "${\\rm supp}(\\beta) = \\{j = 1\\,\\ldots,p : \\beta_j \\ne 0 \\}$ (${\\rm supp}(\\beta)$ is the support of $\\beta$).\n",
    "\n",
    "1. (5 points) Write the subset selection problem in the following form\n",
    "$$\n",
    "\\min_{S \\subseteq \\{1,\\ldots,p\\}, |S|\\le k} y^\\top P_S y,\n",
    "$$\n",
    "where $P_S$ is a projection.  \n",
    "2. (10 points) Suppose that we have a nested sequence of models $S_1\\subset S_2 \\subset \\ldots \\subset S_p$ such that $|S_k| = k$ ($|S_k|$ is the cardinality of $S_k$, meaning that it contains $k$ variables).  Prove that $$y^\\top P_{S_k} y \\ge y^\\top P_{S_{k+1}} y$$ for $k=1,\\ldots,p-1$.  What does this tell us about the solution to the subset selection problem and the constraint $|S| \\le k$?\n",
    "\n",
    "(Hint: using the fact that $X^TX$ is positive definite, write $X^TX= VDV^T$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e8672a",
   "metadata": {},
   "source": [
    "### Exercise 4 (Ridge, lasso and adaptive lasso) (40 pts, 5 pts each)\n",
    "\n",
    "For this exercise, it may be helpful to use the `sklearn.linear_model` module.  I have also included a plotting tool for making the lasso path in ESL.\n",
    "\n",
    "1. Load the training and test data using the script below.  Fit OLS on the training dataset and compute the test error.  Throughout you do not need to compute an intercept but you should normalize the $X$ (divide by the column norms).\n",
    "2. Ridge regression:\n",
    "    - a) Train and tune ridge regression using a validation set (choose LOOCV) and compute the test error (square error loss).\n",
    "    - b) Repeat a) but using K-fold (you can choose $K= 5$ or 10) cross validation, compute the test error. Compare the result to a). Comment on what you found.\n",
    "3. Fit the lasso path with lars to the data and compute the test error for each returned lasso coefficient.\n",
    "4. Perform 3 without the lasso modification generating the lars path.  Compare and contrast the lars path to the lasso path, what is the key difference.  Tell me when the active sets differ and how, if they do at all.\n",
    "5. Fit the lasso path with coordinate descent to the data. Compare the lasso path using coordinate descent with the path using lars. Comment on what you found.\n",
    "6. Extract each active set from the lasso path and recompute the restricted OLS for each.  Compute and compare the test error for each model.\n",
    "7. Read this website [click here](https://towardsdatascience.com/an-adaptive-lasso-63afca54b80d). Fit the adaptive lasso method to the data. Compare the test error between the adaptive lasso and lasso for each returned coefficient. Comment on what you found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4717250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lars(coefs, lines=False, title=\"Lars Path\"):\n",
    "    \"\"\"\n",
    "    Plot the lasso path where coefs is a matrix - the columns are beta vectors\n",
    "    \"\"\"\n",
    "    xx = np.sum(np.abs(coefs.T), axis=1)\n",
    "    xx /= xx[-1]\n",
    "    plt.plot(xx, coefs.T)\n",
    "    ymin, ymax = plt.ylim()\n",
    "    if lines:\n",
    "        plt.vlines(xx, ymin, ymax, linestyle='dashed')\n",
    "    plt.xlabel('|coef| / max|coef|')\n",
    "    plt.ylabel('Coefficients')\n",
    "    plt.title(title)\n",
    "    plt.axis('tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85814dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('hw1.data','rb') as f:\n",
    "    y_tr,X_tr,y_te,X_te = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
