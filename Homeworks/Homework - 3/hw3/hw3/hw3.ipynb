{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2ba0a96",
   "metadata": {},
   "source": [
    "# STA 208: Homework 3 (Do not distribute)\n",
    "\n",
    "## Due 05/22/2022 midnight (11:59pm)\n",
    "\n",
    "__Instructions:__ \n",
    "\n",
    "1. Submit your homework using one file name ”LastName_FirstName_hw3.html” on canvas. \n",
    "2. The written portions can be either done in markdown and TeX in new cells or written by hand and scanned. Using TeX is strongly preferred. However, if you have scanned solutions for handwriting, you can submit a zip file. Please make sure your handwriting is clear and readable and your scanned files are displayed properly in your jupyter notebook. \n",
    "3. Your code should be readable; writing a piece of code should be compared to writing a page of a book. Adopt the one-statement-per-line rule. Consider splitting a lengthy statement into multiple lines to improve readability. (You will lose one point for each line that does not follow the one-statementper-line rule)\n",
    "4. To help understand and maintain code, you should always add comments to explain your code. (homework with no comments will receive 0 points). For a very long comment, please break it into multiple lines.\n",
    "5. In your Jupyter Notebook, put your answers in new cells after each exercise. You can make as many new cells as you like. Use code cells for code and Markdown cells for text.\n",
    "6. Please make sure to print out the necessary results to avoid losing points. We should not run your code to figure out your answers. \n",
    "7. However, also make sure we are able to open this notebook and run everything here by running the cells in sequence; in case that the TA wants to check the details.\n",
    "8. You will be graded on correctness of your math, code efficiency and succinctness, and conclusions and modelling decisions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6525107",
   "metadata": {},
   "source": [
    "### Exercise 1 (Probabilistic PCA and EM)\n",
    "\n",
    "In Week 5-2, we learned the Probabilistic PCA model proposed by Tipping and Bishop (1999). \n",
    "\n",
    "Consider the model \n",
    "$$\n",
    "x_i = \\mu + \\theta w_i + \\sigma^2 \\varepsilon_i,\n",
    "$$\n",
    "where \n",
    "$\\theta \\in \\mathbb{R}^{p \\times r}$ is the loadings matrix (i.e., $\\theta = VD$),\n",
    "$w_i \\overset{\\text{i.i.d}}{\\sim} N(0, I_r)$, and $\\varepsilon_i \\overset{\\text{i.i.d}}{\\sim} N(0, I_p)$.\n",
    "For now, let's assume $\\mu = 0$ for simplicity.\n",
    "\n",
    "The likelihood function can be written as \n",
    "$$\n",
    "\\mathcal{L}(\\theta, \\sigma^2 | x_1, \\dots, x_n) = \\prod_{i=1}^n \\int N(x_i | \\theta w_i, \\sigma^2 I_p) N(w_i | 0, I_r) dw_i,\n",
    "$$\n",
    "and it's log-likelihood function is given by \n",
    "$$\n",
    "\\ell(\\theta, \\sigma^2 | x_1, \\dots, x_n) = \\sum_{i=1}^n \\log \\int N(x_i | \\theta w_i, \\sigma^2 I_p) N(w_i | 0, I_r) dw_i.\n",
    "$$\n",
    "To derive the MLE, we will use the EM algorithm:\n",
    "\n",
    "1. Derive the expectation-step (E-step) of the EM algorithm\n",
    "2. Obtain the expressions of $\\hat \\theta$ and $\\hat \\sigma^2$ from the maximization step (M-step) \n",
    "3. Describe how you implement the algorithm (e.g., you can write a sudo code). How can you make sure $\\langle \\theta_j, \\theta_k \\rangle = 0$ for any $j \\neq k$ in each iteration? ($\\theta_j$ is the $j$-th column of $\\theta$)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f02eaca",
   "metadata": {},
   "source": [
    "### Exercise 2 (Gaussian mixture model and MCMC)\n",
    "\n",
    "We have discussed how to estimate the Gaussian mixture model using EM in class. In this question, we explore the Bayesian finite Gaussian mixture model using MCMC. \n",
    "\n",
    "Recall the finite Gaussian mixture model with $K$ components:\n",
    "$$\n",
    "p(\\boldsymbol x | \\boldsymbol \\pi, \\boldsymbol \\mu, \\boldsymbol \\sigma^2) = \\prod_{i=1}^n \\sum_{k=1}^K \\pi_k N(x_i | \\mu_i, \\sigma_k^2),\n",
    "$$\n",
    "where $\\boldsymbol x = (x_1, \\dots, x_n)$, $\\boldsymbol \\pi = (\\pi_1, \\dots, \\pi_K)$ is a vector of mixing proportions, $\\boldsymbol \\mu = (\\mu_1, \\dots, \\mu_K)$ and $\\boldsymbol \\sigma^2 = (\\sigma_1^2, \\dots, \\sigma_K^2)$ are means and variances of Gaussian densities (here, $\\mu_k$ and $\\sigma_k$ are scalars). \n",
    "\n",
    "In Bayesian setting, we introduce a latent variable $\\boldsymbol z = (z_1, \\dots, z_n)$, $z_j = k$, $k = 1, \\dots, K$. Then, the model can be written as \n",
    "\n",
    "\\begin{align}\n",
    "& z_i | \\boldsymbol \\pi \\sim \\text{Multinomial}(\\pi_1, \\dots, \\pi_K),\\\\\n",
    "& x_i | z_i = k \\sim N(x_i | \\mu_k, \\sigma^2_k).\n",
    "\\end{align}\n",
    "\n",
    "We consider the following conjugate priors:\n",
    "\n",
    "\\begin{align}\n",
    "& \\boldsymbol \\pi \\sim \\text{Dirichlet}(\\alpha_1, \\dots, \\alpha_K),\\\\\n",
    "& \\mu_k \\sim N(0, \\rho_k\\sigma^2),\\\\\n",
    "& \\sigma_k \\sim \\text{Gamma}(a, b),\n",
    "\\end{align}\n",
    "\n",
    "where $\\alpha_1, \\dots, \\alpha_K, \\rho_k, a, b$ are some hyperparamters which are fixed. Their values can be chosen by the user. For Dirichlet distribution, please check its density function [here](https://en.wikipedia.org/wiki/Dirichlet_distribution).\n",
    "\n",
    "We will develop an MCMC algorithm for the model:\n",
    "\n",
    "1. Derive the posterior distribution $\\pi(\\boldsymbol \\pi | \\boldsymbol x, \\boldsymbol z)$ (hint: Dirichlet prior is conjugate with the multinomial distribution [see \"Table of conjugate distributions\"])(https://en.wikipedia.org/wiki/Conjugate_prior)\n",
    "2. Derive the posterior distribution $\\pi(\\boldsymbol \\sigma_k^{-2} | \\boldsymbol x, \\boldsymbol z)$\n",
    "for each $k = 1, \\dots, K$\n",
    "3. Derive the posterior distribution $\\pi(\\boldsymbol \\mu_k | \\boldsymbol x, \\boldsymbol z, \\sigma_k^{-2})$ for each $k = 1, \\dots, K$\n",
    "4. Write a function, call it ``MCMCGaussianFiniteMixture``, for the MCMC algorithm (choose $K = 3$).\n",
    "\n",
    "The structure of your function could look like this:\n",
    "\n",
    "      MCMCGaussianFiniteMixture(x, T = 5000, K = 3, put those hyperparameters here, you can choose default values)\n",
    "            \n",
    "       # Note: T is the total iterations, K is the number of mixture components     \n",
    "       {\n",
    "            \n",
    "            Step 1: choose initial values for parameters to start the algorithm\n",
    "         \n",
    "            Step 2: for t = 1, ..., T (total iteration) # start MCMC iterations\n",
    "            \n",
    "                Step 2.1 draw \\boldsymbol \\pi from its posterior distriction\n",
    "            \n",
    "                Step 2.2: for k = 1, ... K:\n",
    "                \n",
    "                    Step 2.2.1 draw \\sigma_k^{-2} (or \\sigma_k) from its posterior distribution \n",
    "                \n",
    "                    Step 2.2.2 draw \\mu_k from its posterior distribution\n",
    "                \n",
    "                End the for loop\n",
    "             \n",
    "            Collect values \n",
    "             \n",
    "            End the for loop\n",
    "                \n",
    "            Output: sample draws of mu, sigma, pi\n",
    "      }\n",
    "To obtain draws from the Dirichlet distribution, use the function [`np.random.dirichlet`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.dirichlet.html)\n",
    "\n",
    "5. Fit the ``mouse.csv`` data with the function ``MCMCGaussianFiniteMixture`` you wrote (choose $K = 3$, set the total iteration to be $5,000$ or larger. For those hyperparameters, you can choose $\\alpha_1, \\dots, \\alpha_K = 1, \\rho_k = 1$, $a = 2, b = 2$; other values are possible as well), \n",
    "    - a) plot the MCMC chains for $\\pi_k$, $\\mu_k$ and $\\sigma_k$ for all $k = 1, \\dots, K = 3$ (the title of each plot should include the corresponding variable name)\n",
    "    - b) plot the ACF (autocorrelation) function for each chain. \n",
    "    - c) how many draws do you consider as burnin?\n",
    "    - d) remove those draws you considered as burnin, and obtain the posterior means and 95\\% credible intervals for each quantity. Compare those values with the MLE obtained using EM (i.e., fit the data using the [``GaussianMixture``](`https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture) package). Comment on your findings (e.g., are those values from EM close to the posterior means? Do the 95\\% credible intervals include those values from EM?)\n",
    "    - e) (bonus point: 2 points) Using the MCMC draws you obtained to classify the mouse data (i.e., predict the mouse data and determine which label each data point belongs to). Plot the mouse data with predicted labels using ``red`` color for Left Ear, ``green`` color for Right Ear, and ``blue`` for Head. Compare the plot with one using EM (the plot is given in the Week 5-2's lecture notes)\n",
    "\n",
    "Note: ``BayesianGaussianMixture`` in sklearn [here](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.BayesianGaussianMixture.html#sklearn.mixture.BayesianGaussianMixture) does not use the MCMC algorithm. It uses the so-called _variational Bayes method_, which has not been discussed yet. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a22d90",
   "metadata": {},
   "source": [
    "### Exercise 3 (Kernel density estimation)\n",
    "\n",
    "_If you do not have time to complete this question, you can submit your solution with hw4. You will not lose any penalty points if you choose to do so._\n",
    "\n",
    "The data set ``n90_pol.csv`` contains information on 90 British university students who participated in a psychological experiment designed to look for relationships between the size of different regions of the brain and political views.\n",
    "\n",
    "The variables ``amygdala`` and ``acc`` indicate the volume of two particular brain regions known to be involved in emotions and decision-making, the amygdala and the anterior cingulate cortex; more exactly, these are residuals from the predicted volume, after adjusting for height, sex, and similar anatomical variables. The variable ``orientation`` gives the subjects’ locations on a five-point scale from 1 (very conservative) to 5 (very liberal). orientation is an ordinal but not a metric variable, so scores of 1 and 2 are not necessarily as far apart as scores of 2 and 3.\n",
    "\n",
    "1. Estimate the probability density for the volume of the amygdala. Plot it and report the bandwidth. Repeat this for the volume of the ACC.\n",
    "2. Estimate a joint probability density for the volumes of the amygdala and the ACC. What are the bandwidths? Are they the same as the bandwidths you got in problem 1? Should they be?\n",
    "3. Plot the joint density. Does it suggest the two volumes are statistically independent? Should they be? You may use three dimensions, color, contours, etc., for your plot, but you will be graded, in part, on how easy to read it is. (Hint: Remember that the random variables $X$ and $Y$ are statistically independent when their joint pdf is the product of their marginal pdfs, $p(x, y) = p(x)p(y)$. Think about what the product of your estimated pdfs from question 1 would look like."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
